{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import joblib\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from random import seed\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.metrics import Precision, BinaryAccuracy, Recall\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "def load_dataset():\n",
    "    X_yes = np.load(f'{DATA_PATH}/def_yes_images.npy')\n",
    "    X_no = np.load(f'{DATA_PATH}/def_no_images.npy')\n",
    "    classes = np.load(f'{DATA_PATH}/classes.npy')\n",
    "\n",
    "    print(f'X_yes shape: {X_yes.shape}')\n",
    "    print(f'X_no shape {X_no.shape}')\n",
    "    print(f'classes shape: {classes.shape}')\n",
    "\n",
    "    return X_yes, X_no, classes\n",
    "\n",
    "def reshape_dataset(X_yes, X_no):\n",
    "    X = np.row_stack((X_yes, X_no))\n",
    "    N, SIZE_H, SIZE_V = X.shape\n",
    "    X = np.reshape(X, newshape=(N, SIZE_H * SIZE_V))\n",
    "    print(f'X shape: {X.shape}')\n",
    "    return X\n",
    "\n",
    "def data_augmentation(imgs, classes, aug_per_image, imgs_trans):\n",
    "    new_images, new_classes = [], []\n",
    "    for img, y, i in zip(imgs, classes, range(1, len(imgs) + 1)):\n",
    "        for _ in range(aug_per_image):\n",
    "            tmp_img = imgs_trans(image=img)[\"image\"]\n",
    "            new_images.append(tmp_img)\n",
    "            new_classes.append(y)\n",
    "        if i % 50 == 0: print(f'[!] {i} images agumented...')\n",
    "    print(f'[!] Total of {len(imgs)} images augmented!\\n')\n",
    "    return new_images, new_classes\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    # tp/(fn + tp)\n",
    "    y_true_tmp = y_true.numpy()\n",
    "    y_pred_tmp = np.round(y_pred.numpy())\n",
    "    tp = y_true_tmp[y_true_tmp == y_pred_tmp]\n",
    "    tp = len(tp[tp == 1])\n",
    "    fn = y_pred_tmp[y_true_tmp != y_pred_tmp]\n",
    "    fn = len(fn[fn == 0])\n",
    "    try:\n",
    "        return tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    # tn / (fp + tn)\n",
    "    y_true_tmp = y_true.numpy()\n",
    "    y_pred_tmp = np.round(y_pred.numpy())\n",
    "    tn = y_true_tmp[y_true_tmp == y_pred_tmp]\n",
    "    tn = len(tn[tn == 0])\n",
    "    fp = y_pred_tmp[y_true_tmp != y_pred_tmp]\n",
    "    fp = len(fp[fp == 1])\n",
    "    try:\n",
    "        return tn / (tn + fp)\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "\n",
    "# def balanced_acc(sensitivity, specificity):\n",
    "#     return (sensitivity + specificity) / 2.0\n",
    "\n",
    "def balanced_acc(y_true, y_pred):\n",
    "    return (sensitivity(y_true, y_pred) + specificity(y_true, y_pred)) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_yes, X_no, classes = load_dataset()\n",
    "X = np.row_stack((X_yes, X_no))\n",
    "N, SIZE_H, SIZE_V = X.shape\n",
    "N_CLASSES = len(np.unique(classes))\n",
    "print(f'X shape {X.shape}')\n",
    "print(f'Num classes: {N_CLASSES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = seed(time())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, classes, test_size=0.3, random_state=sd, shuffle=True, stratify=classes)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=sd, shuffle=True, stratify=y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_val shape {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_net_5 = keras.Sequential([\n",
    "    # LAYER 1\n",
    "    Conv2D(filters=32, kernel_size=(5, 5), strides=2, padding=\"same\", input_shape=(SIZE_H, SIZE_V, 1), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(4, 4), strides=1, padding=\"valid\"),\n",
    "    # LAYER 2\n",
    "    Conv2D(filters=64, kernel_size=(5, 5), strides=1, padding=\"same\", activation='relu'),\n",
    "    MaxPooling2D(pool_size=(3, 3), strides=2, padding=\"valid\"),\n",
    "    # FLATTEN\n",
    "    Flatten(),\n",
    "    # DENSE LAYERS\n",
    "    Dense(units=120, activation='relu'),\n",
    "    Dense(units=84, activation='relu'),\n",
    "    # OUTPUT\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "IMG_PATH = './images'\n",
    "print(le_net_5.summary())\n",
    "plot_model(le_net_5, to_file=f'{IMG_PATH}/le_net_5.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_16 = keras.Sequential([\n",
    "    # LAYER 1\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', input_shape=(SIZE_H, SIZE_V, 1), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    # LAYER 2\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    # LAYER 3\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    # LAYER 4\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    # LAYER 5\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
    "    # MAXPOOL BEFORE FC\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    # FULLY CONNECTED LAYERS\n",
    "    Flatten(),\n",
    "    Dense(units=4096, activation='relu'),\n",
    "    Dense(units=1000, activation='relu'),\n",
    "    # OUTPUT LAYER\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(vgg_16.summary())\n",
    "plot_model(vgg_16, to_file=f'{IMG_PATH}/vgg_16.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_net = keras.Sequential([\n",
    "    # LAYER 1\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', input_shape=(SIZE_H, SIZE_V, 1), activation='elu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    # LAYER 2\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    # LAYER 3\n",
    "    Conv2DTranspose(filters=64, kernel_size=(2, 2), strides=2, padding='same', activation='elu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    # LAYER 4\n",
    "    Conv2DTranspose(filters=32, kernel_size=(2, 2), strides=2, padding='same', activation='elu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='elu'),\n",
    "    # Fully connected layers\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='sigmoid'),\n",
    "    Dense(units=128, activation='sigmoid'),\n",
    "    # OUTPUT\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(lu_net.summary())\n",
    "plot_model(lu_net, to_file=f'{IMG_PATH}/lu_net.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = keras.optimizers.Adam(learning_rate=0.0003)\n",
    "METRICS = [\n",
    "    Precision(),\n",
    "    BinaryAccuracy(),\n",
    "    Recall(),\n",
    "    sensitivity,\n",
    "    specificity,\n",
    "    balanced_acc\n",
    "]\n",
    "\n",
    "le_net_5.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "vgg_16.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "lu_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(model, X_train, X_test, X_val, y_val, y_train, y_test, batch_size=32, epochs=100):\n",
    "    res_model = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
    "    scores_model = model.evaluate(X_test, y_test)\n",
    "    return res_model, scores_model\n",
    "\n",
    "def graph_history(df, metrics):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(metrics), sharex=True, figsize=(30, 10))\n",
    "    for i, metric in zip(range(len(metrics)), metrics):\n",
    "        ax[i].plot(range(1, 101), df[metric], 'b-', label='Train')\n",
    "        ax[i].plot(range(1, 101), df[f'val_{metric}'], 'r-', label='Validation')\n",
    "        ax[i].set_title(metric)\n",
    "        ax[i].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_le_net, scores_le_net = fit_evaluate(le_net_5, X_train, X_test, X_val, y_val, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le_net_5 = pd.DataFrame(res_le_net.history)\n",
    "df_le_net_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_1, metrics_2 = ['precision', 'binary_accuracy', 'recall'], ['sensitivity', 'specificity', 'balanced_acc']\n",
    "graph_history(df_le_net_5, metrics_1)\n",
    "graph_history(df_le_net_5, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_vgg_16, scores_vgg_16 = fit_evaluate(vgg_16, X_train, X_test, X_val, y_val, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg_16 = pd.DataFrame(res_vgg_16.history)\n",
    "df_vgg_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_history(df_vgg_16, metrics_1)\n",
    "graph_history(df_vgg_16, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lu_net, scores_lu_net = fit_evaluate(lu_net, X_train, X_test, X_val, y_val, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lu_net = pd.DataFrame(res_lu_net.history)\n",
    "df_lu_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_history(df_lu_net, metrics_1)\n",
    "graph_history(df_lu_net, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [scores_le_net, scores_vgg_16, scores_lu_net]\n",
    "losses, precisions = [score[0] for score in scores], [score[1] for score in scores]\n",
    "bin_accs, recalls = [score[2] for score in scores], [score[3] for score in scores]\n",
    "sensitivities, specificities = [score[4] for score in scores], [score[5] for score in scores]\n",
    "baccs = [score[6] for score in scores]\n",
    "\n",
    "scores_df = pd.DataFrame({\n",
    "    'Loss':losses, \n",
    "    'Precision':precisions,\n",
    "    'Binary Accuracy':bin_accs,\n",
    "    'Recall':recalls,\n",
    "    'Sensitivity':sensitivities,\n",
    "    'Specificity':specificities,\n",
    "    'Balanced Accuracy':baccs\n",
    "}, index=['Le-Net 5', 'VGG-16', 'Lu-Net'])\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_PATH = './results'\n",
    "scores_df.to_csv(f'{RES_PATH}/test_scores_1.csv')\n",
    "df_le_net_5.to_csv(f'{RES_PATH}/le_net_5_metrics_1.csv')\n",
    "df_vgg_16.to_csv(f'{RES_PATH}/vgg_16_metrics_1.csv')\n",
    "df_lu_net.to_csv(f'{RES_PATH}/lu_net_metrics_1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_yes, X_no, classes = load_dataset()\n",
    "# X = np.row_stack((X_yes, X_no))\n",
    "N, SIZE_H, SIZE_V = X.shape\n",
    "N_CLASSES = len(np.unique(classes))\n",
    "print(f'X shape {X.shape}')\n",
    "print(f'Num classes: {N_CLASSES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_trans = A.Compose([\n",
    "    A.HorizontalFlip(p=0.25),\n",
    "    A.VerticalFlip(p=0.25),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), p=0.25),\n",
    "    A.Rotate(limit=[-20, 20], p=0.25, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.Affine(translate_percent=(-0.05, 0.05))\n",
    "])\n",
    "\n",
    "X_train, y_train = data_augmentation(X_train, y_train, 20, imgs_trans)\n",
    "X_val, y_val = data_augmentation(X_val, y_val, 20, imgs_trans)\n",
    "X_test, y_test = data_augmentation(X_test, y_test, 20, imgs_trans)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_val shape {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n",
    "\n",
    "# X_aug, y_aug = data_augmentation(X, classes, 20, imgs_trans)\n",
    "# X_aug, y_aug = np.array(X_aug), np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, SIZE_H, SIZE_V = X_aug.shape\n",
    "# N_CLASSES = len(np.unique(y_aug))\n",
    "# print(f'X shape {X_aug.shape}')\n",
    "# print(f'Num classes: {N_CLASSES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd = seed(time())\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, test_size=0.3, random_state=sd, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_le_net, scores_le_net = fit_evaluate(le_net_5, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le_net_5 = pd.DataFrame(res_le_net.history)\n",
    "df_le_net_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_1, metrics_2 = ['precision', 'binary_accuracy', 'recall'], ['sensitivity', 'specificity', 'balanced_acc']\n",
    "graph_history(df_le_net_5, metrics_1)\n",
    "graph_history(df_le_net_5, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_PATH = './results'\n",
    "df_le_net_5.to_csv(f'{RES_PATH}/le_net_5_metrics_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_vgg_16, scores_vgg_16 = fit_evaluate(vgg_16, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg_16 = pd.DataFrame(res_vgg_16.history)\n",
    "df_vgg_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_history(df_vgg_16, metrics_1)\n",
    "graph_history(df_vgg_16, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg_16.to_csv(f'{RES_PATH}/vgg_16_metrics_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lu_net, scores_lu_net = fit_evaluate(lu_net, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lu_net = pd.DataFrame(res_lu_net.history)\n",
    "df_lu_net.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_history(df_lu_net, metrics_1)\n",
    "graph_history(df_lu_net, metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lu_net.to_csv(f'{RES_PATH}/lu_net_metrics_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [scores_le_net, scores_vgg_16, scores_lu_net]\n",
    "losses, precisions = [score[0] for score in scores], [score[1] for score in scores]\n",
    "bin_accs, recalls = [score[2] for score in scores], [score[3] for score in scores]\n",
    "sensitivities, specificities = [score[4] for score in scores], [score[5] for score in scores]\n",
    "baccs = [balanced_acc(sens, spec) for sens, spec in zip(sensitivities, specificities)]\n",
    "\n",
    "scores_df = pd.DataFrame({\n",
    "    'Loss':losses, \n",
    "    'Precision':precisions,\n",
    "    'Binary Accuracy':bin_accs,\n",
    "    'Recall':recalls,\n",
    "    'Sensitivity':sensitivities,\n",
    "    'Specificity':specificities,\n",
    "    'Balanced Accuracy':baccs\n",
    "})\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv(f'{RES_PATH}/test_scores_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12749f567798517b8543354a13719bbd42e9e3e56a89ba27a040f4f72d5c2230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
